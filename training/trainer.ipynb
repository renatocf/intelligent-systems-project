{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-panama",
   "metadata": {},
   "source": [
    "# Development of Intelligent Computing Systems _ 2021  IME-USP\n",
    "- course [page][3]\n",
    "- ministred by: MSc [Renato Cordeiro Ferreira][1]\n",
    "- student: [Rodrigo Didier Anderson][2]\n",
    "\n",
    "[1]: https://www.linkedin.com/in/renatocf/\n",
    "[2]: https://www.linkedin.com/in/didier11/\n",
    "[3]: https://www.ime.usp.br/verao/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-friendly",
   "metadata": {},
   "source": [
    "This is the first part of the course project, we will create the training pipeline for a categorization model.\n",
    "\n",
    "More specifically, the goal is to train a model that should receive data related to products and return the best categories for them.\n",
    "\n",
    "\n",
    "- More details about this stage of the project [here][1].\n",
    "- More info about the data can be found [here][2]\n",
    "[1]: https://github.com/didier-rda/intelligent-systems-project/blob/main/training/README.md\n",
    "[2]: https://github.com/didier-rda/intelligent-systems-project/blob/main/data/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-johnson",
   "metadata": {},
   "source": [
    "## Training Pipeline  \n",
    "(less than 5 minutes)\n",
    "\n",
    "This training pipeline follows the following steps. \n",
    "\n",
    "For each step, a class was created with the necessary methods to fulfill the respective stage of the pipline.\n",
    "\n",
    "1. **Data extraction** <br>\n",
    "   Loads a dataset with product data from a specified path available in the\n",
    "   environment variable `DATASET_PATH`.\n",
    "   \n",
    "   class: `dataExtractor`\n",
    "\n",
    "\n",
    "\n",
    "2. **Data formatting** <br>\n",
    "   Processes the dataset to use it for training and validation.\n",
    "   \n",
    "   class: `dataFormatter`\n",
    "\n",
    "\n",
    "\n",
    "3. **Data Modeling & Model Exportation** <br>\n",
    "   - Specifies a model to handle the categorization problem;\n",
    "   - Exports a candidate model to a specified path available in the environment\n",
    "     variable `MODEL_PATH`;\n",
    "   \n",
    "   class: `dataModeler`\n",
    "\n",
    "\n",
    "\n",
    "4. **Model validation** <br>\n",
    "   Generates metrics about the model accuracy (precision, recall, F1, etc.)\n",
    "   for each category and exports them to a specified path available in the\n",
    "   environment variable `METRICS_PATH`.\n",
    "\n",
    "   class: `modelValidator`\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "for the pipeline scheduling a last class: `dataPipeline` was created.\n",
    "\n",
    "It purpose is to run the pipeline recursively by calling the other classes when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-principle",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "combined-saver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# sys and data processing libs\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nlp data corpus and libs\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from scipy import sparse as sp_sparse\n",
    "\n",
    "# ML modules from sklearn\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-injury",
   "metadata": {},
   "source": [
    "# 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "speaking-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.data_path = os.getenv('DATASET_PATH')\n",
    "        self.data = self.extractData()\n",
    "\n",
    "        \n",
    "    def extractData(self):\n",
    "        '''\n",
    "        Loads a dataset with product data from a specified path.\n",
    "        '''\n",
    "       \n",
    "        return pd.read_csv(self.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-portable",
   "metadata": {},
   "source": [
    "#  2. Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funky-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataFormatter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.extracted_data = dataExtractor().data\n",
    " \n",
    "\n",
    "    def formatData(self):\n",
    "        '''\n",
    "        Processes the dataset to use it for training and validation.\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    def categoryToDummy(self):\n",
    "        # get dummy\n",
    "        dummy = pd.get_dummies(self.extracted_data['category'])\n",
    "\n",
    "        # merge df\n",
    "        df_dummy = pd.merge(self.extracted_data, dummy, left_index=True, right_index=True)\n",
    "\n",
    "        # delet no dummy cols\n",
    "        del df_dummy['category']\n",
    "\n",
    "        return df_dummy\n",
    "    \n",
    "    \n",
    "    def splitTrainTestValidate(self):\n",
    "        '''\n",
    "        Split the dataset into train, validation and test\n",
    "        \n",
    "        train: 60%\n",
    "        \n",
    "        test: 20%\n",
    "        \n",
    "        validatiom: 20%\n",
    "        '''\n",
    "        \n",
    "        # categorical variables list to split\n",
    "        y_cols = list(self.categoryToDummy().columns[-6:])\n",
    "        X_cols = list(self.categoryToDummy().columns[:-6])\n",
    "        \n",
    "        # full data categorized\n",
    "        y_data =  self.categoryToDummy()[y_cols]\n",
    "        X_data =  self.categoryToDummy()[X_cols]\n",
    "                \n",
    "        X, X_test, y, y_test = train_test_split(X_data, y_data, test_size=0.2, train_size=0.8)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.25,train_size =0.75)\n",
    "        \n",
    "        return (X, y), (X_test, y_test), (X_val, y_val)\n",
    "  \n",
    "\n",
    "    def joinStringColumns(self):\n",
    "        \n",
    "        (X, y), (X_test, y_test), (X_val, y_val) = self.splitTrainTestValidate()\n",
    "        f_join_strings = lambda row: row['query'] + ' ' + row['title'] + ' ' + row['concatenated_tags']\n",
    "        \n",
    "        X = X.assign(full_text= f_join_strings)\n",
    "        X_test = X_test.assign(full_text= f_join_strings)\n",
    "        X_val = X_val.assign(full_text= f_join_strings)\n",
    "        \n",
    "        return (X, y), (X_test, y_test), (X_val, y_val)\n",
    "    \n",
    " \n",
    "    def normalizeFullText(self):\n",
    "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    \n",
    "        \n",
    "        def normalize_text(s):\n",
    "            \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"        \n",
    "\n",
    "            \n",
    "            def compost_words(text):\n",
    "                text = re.sub('[/]', ' ', str(text))\n",
    "                text = re.sub('[-]', ' ', str(text))\n",
    "                return text\n",
    "\n",
    "            \n",
    "            def white_space_fix(text):\n",
    "                return \" \".join(text.split())\n",
    "\n",
    "            \n",
    "            def remove_punc(text):\n",
    "                exclude = set(punctuation)\n",
    "                return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "            \n",
    "            def lower(text):\n",
    "                return text.lower()\n",
    "    \n",
    "    \n",
    "            def remove_stop_words(text):\n",
    "                from nltk.corpus import stopwords # this import in this part of the code was necessary to not give an error\n",
    "                stopwords = set(stopwords.words('portuguese') + list(punctuation) + list('/'))\n",
    "                palavras = word_tokenize(text)\n",
    "                palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords]\n",
    "                return \" \".join(palavras_sem_stopwords)\n",
    "        \n",
    "            return remove_stop_words(white_space_fix(remove_punc(lower(compost_words(s)))))\n",
    "    \n",
    "        (X, y), (X_test, y_test), (X_val, y_val) = self.joinStringColumns()\n",
    "\n",
    "        X['full_text'] = list(map(normalize_text,list(X['full_text'])))\n",
    "        X_test['full_text'] = list(map(normalize_text,list(X_test['full_text'])))\n",
    "        X_val['full_text'] = list(map(normalize_text,list(X_val['full_text'])))\n",
    "        \n",
    "        \n",
    "        return (X, y), (X_test, y_test), (X_val, y_val)\n",
    "\n",
    "    \n",
    "    def getBagOfWordsSparse(self):\n",
    "        '''\n",
    "        represent words in a corpus in a numeric format for multilabel classification.\n",
    "        '''\n",
    "        (X, y), (X_test, y_test), (X_val, y_val) = self.normalizeFullText()\n",
    "        \n",
    "        # start counting words in trainig data\n",
    "        words_counts = {}\n",
    "        for text in X.full_text:\n",
    "            for word in text.split():\n",
    "                words_counts[word] = 1\n",
    "            words_counts[word] += 1\n",
    "    \n",
    "        # get 10k most popular words - to decrease the complexity of processing\n",
    "        DICT_SIZE = int(os.getenv('DICT_OF_WORDS_SIZE'))\n",
    "        POPULAR_WORDS = sorted(words_counts, key=words_counts.get, reverse=True)[:DICT_SIZE]\n",
    "        WORDS_TO_INDEX = {key: rank for rank, key in enumerate(POPULAR_WORDS, 0)}\n",
    "        INDEX_TO_WORDS = {index:word for word, index in WORDS_TO_INDEX.items()}\n",
    "        ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "        \n",
    "        def my_bag_of_words(text, words_to_index, dict_size):\n",
    "            \"\"\"\n",
    "            text: a string\n",
    "            dict_size: size of the dictionary\n",
    "        \n",
    "            return a vector which is a bag-of-words representation of 'text'\n",
    "            \"\"\"\n",
    "            result_vector = np.zeros(dict_size)\n",
    "            for word in text.split(' '):\n",
    "                if word in words_to_index:\n",
    "                    result_vector[words_to_index[word]] +=1\n",
    "            return result_vector\n",
    "\n",
    "        X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X.full_text])\n",
    "        X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test.full_text])\n",
    "        X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val.full_text])\n",
    "        \n",
    "        return (X_train_mybag, y), (X_test_mybag, y_test), (X_val_mybag, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-gilbert",
   "metadata": {},
   "source": [
    "# 3. Data Modeling & Model Exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "whole-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataModeler:\n",
    "     \n",
    "    def createModelBoWClassifier(self):\n",
    "        '''\n",
    "        Specifies a model to handle the categorization problem.\n",
    "        # criar etapa de modelagem -métodos se for o caso\n",
    "        # dar split no train e treinar     \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        def train_classifier(X_train, y_train, C, regularisation):\n",
    "            \"\"\"\n",
    "            X_train, y_train — training data\n",
    "      \n",
    "            return: trained classifier\n",
    "            \"\"\"\n",
    "    \n",
    "            # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "            model = OneVsRestClassifier(LogisticRegression(penalty=regularisation, C=C, max_iter=10000)).fit(X_train, y_train)\n",
    "            return model\n",
    "        \n",
    "        (X_train_mybag, y), (X_test_mybag, y_test), (X_val_mybag, y_val) = dataFormatter().getBagOfWordsSparse()\n",
    "        \n",
    "        classifier_bow = train_classifier(X_train_mybag, y, C = 4, regularisation = 'l2')\n",
    "        \n",
    "        # in this first version, model 'll be saved here in the code.\n",
    "        pickle.dump(classifier_bow, open(os.getenv('MODEL_PATH'),'wb'))\n",
    "        \n",
    "        pickle.dump(X_train_mybag, open(os.getenv('X_TRAIN'),'wb'))\n",
    "        pickle.dump(y, open(os.getenv('Y_TRAIN'),'wb'))\n",
    "        pickle.dump(X_test_mybag, open(os.getenv('X_TEST'),'wb'))\n",
    "        pickle.dump(y_test, open(os.getenv('Y_TEST'),'wb'))\n",
    "        pickle.dump(X_val_mybag, open(os.getenv('X_VAL'),'wb'))\n",
    "        pickle.dump(y_val, open(os.getenv('Y_VAL'),'wb'))\n",
    "        return classifier_bow\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-relaxation",
   "metadata": {},
   "source": [
    "# 4. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confidential-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelValidator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = dataModeler().createModelBoWClassifier()\n",
    "        self.target_names = ['Bebê','Bijuterias e Jóias','Decoração','Lembrancinhas','Outros','Papel e Cia']\n",
    "     \n",
    "    \n",
    "    def validateModel(self):\n",
    "        '''\n",
    "        Generates metrics about the model accuracy (precision, recall, F1, etc.)\n",
    "        for each category and exports them to a specified path available in the \n",
    "        environment variable METRICS_PATH.\n",
    "        '''\n",
    "\n",
    "        def write_evaluation_scores(y_test, y_test_predicted, y_val, y_val_predicted):\n",
    "            \n",
    "            f = open(os.getenv('METRICS_PATH'), \"w\")\n",
    "            f.write(f\"model BoW TEST metrics:\\n\")\n",
    "            f.write(f\"\\n{classification_report(y_test, y_test_predicted, target_names=self.target_names, zero_division=0)}\\n\")\n",
    "            f.write(f\"\\nmodel BoW VALIDATION metrics:\\n\")\n",
    "            f.write(f\"\\n{classification_report(y_val, y_val_predicted, target_names=self.target_names, zero_division=0)}\\n\")\n",
    "            f.close()\n",
    "        \n",
    "        X_test_mybag = pickle.load(open(os.getenv('X_TEST'),'rb'))\n",
    "        y_test = pickle.load(open(os.getenv('Y_TEST'),'rb'))\n",
    "        X_val_mybag = pickle.load(open(os.getenv('X_VAL'),'rb'))\n",
    "        y_val = pickle.load(open(os.getenv('Y_VAL'),'rb'))\n",
    "        \n",
    "        y_test_predicted = self.model.predict(X_test_mybag)\n",
    "        y_val_predicted = self.model.predict(X_val_mybag)\n",
    "        \n",
    "        write_evaluation_scores(y_test, y_test_predicted, y_val, y_val_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-fancy",
   "metadata": {},
   "source": [
    "# 5. DS Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "russian-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataPipeline:\n",
    "    def __init__(self):\n",
    "        self.run = modelValidator().validateModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "streaming-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPipeline().run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
